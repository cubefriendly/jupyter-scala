{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark demo\n",
    "\n",
    "Illustrates how to use the ammonite-spark library, that adds Spark support to (my [fork](https://github.com/alexarchambault/ammonite-shell) of) the ammonite REPL or a notebook.\n",
    "\n",
    "The examples come mostly from the spark-repl test suite: https://github.com/apache/spark/blob/master/repl/scala-2.11/src/test/scala/org/apache/spark/repl/ReplSuite.scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from a bare scala 2.10 jupyter scala notebook. First, let's fetch ammonite-spark.\n",
    "\n",
    "Notice the `1.3` in the dependency name, which corresponds to our spark version, and the `2.10.5` which is the full current scala version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "test_ignore": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::: WARNINGS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: problems summary ::"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-scala-api_2.11.6;0.2.0-SNAPSHOT from sonatype-snapshots, using Mon Jun 01 15:32:30 CEST 2015\n",
      "\n",
      "\tSorting results from com.github.alexarchambault.jupyter#jupyter-scala-api_2.11.6;0.2.0-SNAPSHOT, using Mon Jun 01 15:32:30 CEST 2015 and null\n",
      "\n",
      "\tSorting results from com.github.alexarchambault.jupyter#jupyter-scala-api_2.11.6;0.2.0-SNAPSHOT, using Mon Jun 01 10:20:55 CEST 2015 and Mon Jun 01 10:20:55 CEST 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-scala-api_2.11.6;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-api_2.11.6;0.3.1-SNAPSHOT from sonatype-snapshots, using Mon Jun 01 14:46:12 CEST 2015\n",
      "\n",
      "\tSorting results from com.github.alexarchambault#ammonite-api_2.11.6;0.3.1-SNAPSHOT, using Mon Jun 01 14:46:12 CEST 2015 and null\n",
      "\n",
      "\tSorting results from com.github.alexarchambault#ammonite-api_2.11.6;0.3.1-SNAPSHOT, using Mon Jun 01 10:18:43 CEST 2015 and Mon Jun 01 10:18:43 CEST 2015\n",
      "\n",
      "\tChoosing sonaty"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe-snapshots for com.github.alexarchambault#ammonite-api_2.11.6;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-api_2.11;0.2.0-SNAPSHOT from sonatype-snapshots, using Mon Jun 01 02:54:01 CEST 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-api_2.11;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-spark_1.3_2.11.6;0.3.1-SNAPSHOT from sonatype-snapshots, using Mon Jun 01 14:46:23 CEST 2015\n",
      "\n",
      "\tSorting results from com.github.alexarchambault#ammonite-spark_1.3_2.11.6;0.3.1-SNAPSHOT, using Mon Jun 01 14:46:23 CEST 2015 and null\n",
      "\n",
      "\tSorting results from com.github.alexarchambault#ammonite-spark_1.3_2.11.6;0.3.1-SNAPSHOT, using Mon Jun 01 10:19:06 CEST 2015 and Mon Jun 01 10:19:06 CEST 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-spark_1.3_2.11.6;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-shell-api_2.11.6;0.3.1-SNAPSHOT from sonatype-snapshots, using Mon Jun 01 14:46:14 CEST 2015\n",
      "\n",
      "\tSorting results from com.github.alexarchambault#ammonite-shell-api_2.11.6;0.3.1-SNAPSHOT, using Mon Jun 01 14:46:14 CEST 2015 and null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tSorting results from com.github.alexarchambault#ammonite-shell-api_2.11.6;0.3.1-SNAPSHOT, using Mon Jun 01 10:18:43 CEST 2015 and Mon Jun 01 10:18:43 CEST 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-shell-api_2.11.6;0.3.1-SNAPSHOT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load.ivy(\"com.github.alexarchambault\" % \"ammonite-spark_1.3_2.11.6\" % \"0.3.1-SNAPSHOT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a handle, of type `ammonite.spark.Spark`. Upon creation, it gets an implicit `ammonite.api.Interpreter`, that one can find when running code from a notebook, and uses it later to initialize spark.\n",
    "\n",
    "Note the `@transient` annotation added to it, so that it will not get serialized when running closures on a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "test_drop_stderr": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mSpark\u001b[0m: \u001b[32mammonite\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mSpark\u001b[0m = Spark(uninitialized)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@transient val Spark = new ammonite.spark.Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `SparkContext` is accessible through the `sc` method of the spark handle. It is *lazily* initialized, which means it is not yet, as we didn't call the `sc` method.\n",
    "\n",
    "Before that, we'll setup the spark config, through a `SparkConf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Spark.withConf(_\n",
    "  .setMaster(\"local\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mSpark.sc\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Spark.sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "test_drop_stderr": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "15/06/01 19:38:03 INFO Spark$SparkContext: Running Spark version 1.3.1\n",
      "15/06/01 19:38:04 WARN Utils: Your hostname, pc-ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.0.15 instead (on interface eth0)\n",
      "15/06/01 19:38:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "15/06/01 19:38:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/06/01 19:38:04 INFO SecurityManager: Changing view acls to: alexandre\n",
      "15/06/01 19:38:04 INFO SecurityManager: Changing modify acls to: alexandre\n",
      "15/06/01 19:38:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(alexandre); users with modify permissions: Set(alexandre)\n",
      "15/06/01 19:38:04 INFO Slf4jLogger: Slf4jLogger started\n",
      "15/06/01 19:38:04 INFO Remoting: Starting remoting\n",
      "15/06/01 19:38:04 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.15:59425]\n",
      "15/06/01 19:38:04 INFO Utils: Successfully started service 'sparkDriver' on port 59425.\n",
      "15/06/01 19:38:04 INFO SparkEnv: Registering MapOutputTracker\n",
      "15/06/01 19:38:04 INFO SparkEnv: Registering BlockManagerMaster\n",
      "15/06/01 19:38:04 INFO DiskBlockManager: Created local directory at /tmp/spark-b7648766-10d6-4fa6-bfff-2f18b270a61d/blockmgr-348d4dff-4491-4208-a110-1c431e4deb05\n",
      "15/06/01 19:38:04 INFO MemoryStore: MemoryStore started with capacity 943.9 MB\n",
      "15/06/01 19:38:04 INFO HttpFileServer: HTTP File server directory is /tmp/spark-34db488b-c354-4055-a3e4-d213f98e4ec2/httpd-392f965e-f2b1-4d9e-a3e4-bea344fd08eb\n",
      "15/06/01 19:38:04 INFO HttpServer: Starting HTTP Server\n",
      "15/06/01 19:38:04 INFO Server: jetty-8.y.z-SNAPSHOT\n",
      "15/06/01 19:38:04 INFO AbstractConnector: Started SocketConnector@0.0.0.0:35901\n",
      "15/06/01 19:38:04 INFO Utils: Successfully started service 'HTTP file server' on port 35901.\n",
      "15/06/01 19:38:04 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "15/06/01 19:38:09 INFO Server: jetty-8.y.z-SNAPSHOT\n",
      "15/06/01 19:38:09 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4041\n",
      "15/06/01 19:38:09 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "15/06/01 19:38:09 INFO SparkUI: Started SparkUI at http://192.168.0.15:4041\n",
      "15/06/01 19:38:09 INFO Spark$SparkContext: Added JAR file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/ammonite-api_2.11.6-0.3.1-SNAPSHOT.jar at http://192.168.0.15:35901/jars/ammonite-api_2.11.6-0.3.1-SNAPSHOT.jar with timestamp 1433180289831\n",
      "15/06/01 19:38:09 INFO Spark$SparkContext: Added JAR file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/jupyter-api_2.11-0.2.0-SNAPSHOT.jar at http://192.168.0.15:35901/jars/jupyter-api_2.11-0.2.0-SNAPSHOT.jar with timestamp 1433180289831\n",
      "15/06/01 19:38:09 INFO Spark$SparkContext: Added JAR file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/jupyter-scala-api_2.11.6-0.2.0-SNAPSHOT.jar at http://192.168.0.15:35901/jars/jupyter-scala-api_2.11.6-0.2.0-SNAPSHOT.jar with timestamp 1433180289831\n",
      "15/06/01 19:38:09 INFO Spark$SparkContext: Added JAR file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/ammonite-pprint_2.11-0.3.2.jar at http://192.168.0.15:35901/jars/ammonite-pprint_2.11-0.3.2.jar with timestamp 1433180289897\n",
      "15/06/01 19:38:09 INFO Spark$SparkContext: Added JAR file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scala-library-2.11.6.jar at http://192.168.0.15:35901/jars/scala-library-2.11.6.jar with timestamp 1433180289902\n",
      "15/06/01 19:38:09 INFO Spark$SparkContext: Added JAR file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scala-reflect-2.11.6.jar at http://192.168.0.15:35901/jars/scala-reflect-2.11.6.jar with timestamp 1433180289905\n",
      "15/06/01 19:38:09 INFO Spark$SparkContext: Added JAR file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scala-xml_2.11-1.0.3.jar at http://192.168.0.15:35901/jars/scala-xml_2.11-1.0.3.jar with timestamp 1433180289905\n",
      "15/06/01 19:38:09 INFO Spark$SparkContext: Added JAR file:/home/alexandre/.ivy2/cache/com.github.alexarchambault/ammonite-spark_1.3_2.11.6/jars/ammonite-spark_1.3_2.11.6-0.3.1-SNAPSHOT.jar at http://192.168.0.15:35901/jars/ammonite-spark_1.3_2.11.6-0.3.1-SNAPSHOT.jar with timestamp 1433180289906\n",
      "15/06/01 19:38:09 INFO Spark$SparkContext: Added JAR file:/home/alexandre/.ivy2/cache/com.github.alexarchambault/ammonite-shell-api_2.11.6/jars/ammonite-shell-api_2.11.6-0.3.1-SNAPSHOT.jar at http://192.168.0.15:35901/jars/ammonite-shell-api_2.11.6-0.3.1-SNAPSHOT.jar with timestamp 1433180289998\n",
      "15/06/01 19:38:09 INFO Spark$SparkContext: Added JAR file:/home/alexandre/.ivy2/cache/org.eclipse.jetty/jetty-server/jars/jetty-server-8.1.14.v20131031.jar at http://192.168.0.15:35901/jars/jetty-server-8.1.14.v20131031.jar with timestamp 1433180289999\n",
      "15/06/01 19:38:09 INFO Spark$SparkContext: Added JAR file:/home/alexandre/.ivy2/cache/org.eclipse.jetty/jetty-continuation/jars/jetty-continuation-8.1.14.v20131031.jar at http://192.168.0.15:35901/jars/jetty-continuation-8.1.14.v20131031.jar with timestamp 1433180289999\n",
      "15/06/01 19:38:09 INFO Spark$SparkContext: Added JAR file:/home/alexandre/.ivy2/cache/org.eclipse.jetty/jetty-http/jars/jetty-http-8.1.14.v20131031.jar at http://192.168.0.15:35901/jars/jetty-http-8.1.14.v20131031.jar with timestamp 1433180289999\n",
      "15/06/01 19:38:10 INFO Spark$SparkContext: Added JAR file:/home/alexandre/.ivy2/cache/org.eclipse.jetty/jetty-io/jars/jetty-io-8.1.14.v20131031.jar at http://192.168.0.15:35901/jars/jetty-io-8.1.14.v20131031.jar with timestamp 1433180290100\n",
      "15/06/01 19:38:10 INFO Spark$SparkContext: Added JAR file:/home/alexandre/.ivy2/cache/org.eclipse.jetty/jetty-util/jars/jetty-util-8.1.14.v20131031.jar at http://192.168.0.15:35901/jars/jetty-util-8.1.14.v20131031.jar with timestamp 1433180290100\n",
      "15/06/01 19:38:10 INFO Executor: Starting executor ID <driver> on host localhost\n",
      "15/06/01 19:38:10 INFO Executor: Using REPL class URI: http://127.0.1.1:40903\n",
      "15/06/01 19:38:10 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.15:59425/user/HeartbeatReceiver\n",
      "15/06/01 19:38:10 INFO NettyBlockTransferService: Server created on 41425\n",
      "15/06/01 19:38:10 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "15/06/01 19:38:10 INFO BlockManagerMasterActor: Registering block manager localhost:41425 with 943.9 MB RAM, BlockManagerId(<driver>, localhost, 41425)\n",
      "15/06/01 19:38:10 INFO BlockManagerMaster: Registered BlockManager\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Spark.start() // equivalent to just calling sc, triggers its initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36maccum\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mAccumulator\u001b[0m[\u001b[32mInt\u001b[0m] = 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val accum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.parallelize(1 to 10).foreach(x => accum += x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres7\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m55\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mdouble\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def double(x: Int) = x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres9\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m110\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.parallelize(1 to 10).map(x => double(x)).collect().reduceLeft(_+_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutable variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mv\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m7\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var v = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mgetV\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getV() = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres12\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m70\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.parallelize(1 to 10).map(x => getV()).collect().reduceLeft(_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres14\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m100\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.parallelize(1 to 10).map(x => getV()).collect().reduceLeft(_+_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining your own classes is ok\n",
    "\n",
    "and more complex definions should too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mSum\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Sum(exp: String, exp2: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36ma\u001b[0m: \u001b[32mSum\u001b[0m = \u001b[33mSum\u001b[0m(\u001b[32m\"A\"\u001b[0m, \u001b[32m\"B\"\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val a = Sum(\"A\", \"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mb\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def b(a: Sum): String = a match { case Sum(_, _) => \"Found Sum\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres18\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"Found Sum\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSQL :|\n",
    "\n",
    "Spark SQL doesn't work from here because of https://issues.apache.org/jira/browse/SPARK-5281. It should be fixed in the next Spark releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mSpark.sqlContext\u001b[0m\n",
       "\u001b[32mimport \u001b[36msqlContext.implicits._\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Spark.sqlContext\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mTestCaseClass\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class TestCaseClass(value: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "ignore_err_message": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "scala.ScalaReflectionException: class org.apache.spark.sql.catalyst.ScalaReflection in JavaMirror with sun.misc.Launcher$AppClassLoader@2503dbd3 of type class sun.misc.Launcher$AppClassLoader with classpath [file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/ammonite-pprint_2.11-0.3.2.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scalaz-concurrent_2.11-7.1.0.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scalaz-stream_2.11-0.6a.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/jeromq-0.3.4.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scala-xml_2.11-1.0.3.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/argonaut_2.11-6.1-M5.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scala-logging-slf4j_2.11-2.1.2.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scala-logging-api_2.11-2.1.2.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/logback-classic-1.0.13.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scalaz-core_2.11-7.1.0.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scalaz-effect_2.11-7.1.0.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/jupyter-api_2.11-0.2.0-SNAPSHOT.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/jupyter-scala-cli_2.11.6-0.2.0-SNAPSHOT.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/jupyter-kernel_2.11-0.2.0-SNAPSHOT.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/argonaut-shapeless_6.1_2.11-0.1.1.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scala-compiler-2.11.6.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/ammonite-api_2.11.6-0.3.1-SNAPSHOT.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/case-app_2.11-0.2.2.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/monocle-core_2.11-0.5.1.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scala-parser-combinators_2.11-1.0.3.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/ivy-light_2.11.6-0.3.1-SNAPSHOT.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/ammonite-interpreter_2.11.6-0.3.1-SNAPSHOT.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/slf4j-api-1.7.7.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scala-library-2.11.6.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scalaparse_2.11-0.1.6-SNAPSHOT.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/config-1.2.1.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/monocle-macro_2.11-0.5.1.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/shapeless_2.11-2.1.0.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/jupyter-scala-api_2.11.6-0.2.0-SNAPSHOT.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/fastparse-utils_2.11-0.1.6-SNAPSHOT.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/paradise_2.11.6-2.0.1.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scala-reflect-2.11.6.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/fastparse_2.11-0.1.6-SNAPSHOT.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/logback-core-1.0.13.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/scodec-bits_2.11-1.0.4.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/ivy-2.4.0.jar,file:/home/alexandre/.ipython/kernels/scala211/jupyter-scala_2.11.6-0.2.0-SNAPSHOT/lib/jupyter-scala_2.11.6-0.2.0-SNAPSHOT.jar] and parent being sun.misc.Launcher$ExtClassLoader@24d09c1 of type class sun.misc.Launcher$ExtClassLoader with classpath [file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunjce_provider.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/cldrdata.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/localedata.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunpkcs11.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/zipfs.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunec.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/jfxrt.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/dnsns.jar,file:/usr/lib/jvm/java-8-oracle/jre/lib/ext/nashorn.jar] and parent being primordial classloader with boot classpath [/usr/lib/jvm/java-8-oracle/jre/lib/resources.jar:/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jsse.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jce.jar:/usr/lib/jvm/java-8-oracle/jre/lib/charsets.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfr.jar:/usr/lib/jvm/java-8-oracle/jre/classes] not found.",
      "\tscala.reflect.internal.Mirrors$RootsBase.staticClass(Mirrors.scala:123)",
      "\tscala.reflect.internal.Mirrors$RootsBase.staticClass(Mirrors.scala:22)",
      "\torg.apache.spark.sql.catalyst.ScalaReflection$$typecreator1$1.apply(ScalaReflection.scala:127)",
      "\tscala.reflect.api.TypeTags$WeakTypeTagImpl.tpe$lzycompute(TypeTags.scala:232)",
      "\tscala.reflect.api.TypeTags$WeakTypeTagImpl.tpe(TypeTags.scala:232)",
      "\tscala.reflect.api.TypeTags$class.typeOf(TypeTags.scala:341)",
      "\tscala.reflect.api.Universe.typeOf(Universe.scala:61)",
      "\torg.apache.spark.sql.catalyst.ScalaReflection$class.schemaFor(ScalaReflection.scala:127)",
      "\torg.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:30)",
      "\torg.apache.spark.sql.catalyst.ScalaReflection$class.schemaFor(ScalaReflection.scala:112)",
      "\torg.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:30)",
      "\torg.apache.spark.sql.SQLContext.createDataFrame(SQLContext.scala:320)",
      "\torg.apache.spark.sql.SQLContext$implicits$.rddToDataFrameHolder(SQLContext.scala:258)",
      "\tcmd21$$user$$anonfun$1.apply(Main.scala:129)",
      "\tcmd21$$user$$anonfun$1.apply(Main.scala:128)"
     ]
    }
   ],
   "source": [
    "sc.parallelize(1 to 10).map(x => TestCaseClass(x)).toDF().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused non serializable things are fine\n",
    "\n",
    "If they are not used in parallel calculations, they do not prevent serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mTestClass\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TestClass() { def testMethod = 3; override def toString = \"TestClass\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mt\u001b[0m: \u001b[32mTestClass\u001b[0m = TestClass"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// not serializable\n",
    "val t = new TestClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mt.testMethod\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import t.testMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mTestCaseClass\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// serializable\n",
    "case class TestCaseClass(value: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres26\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mTestCaseClass\u001b[0m] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m1\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m2\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m3\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m4\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m5\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m6\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m7\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m8\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m9\u001b[0m),\n",
       "  \u001b[33mTestCaseClass\u001b[0m(\u001b[32m10\u001b[0m)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// some parallel calculations with the serializable class\n",
    "sc.parallelize(1 to 10).map(x => TestCaseClass(x)).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
